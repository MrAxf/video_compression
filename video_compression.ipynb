{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video compression\n",
    "\n",
    "## Contents\n",
    "1. [Sources of redundancy in video](#sources).\n",
    "2. [Block-based motion compensation](#MC).\n",
    "3. [Sub-pixel accuracy when estimating motion](#subpixel_accuracy).\n",
    "4. [Block matching criteria](#matching_criteria).\n",
    "5. [Block search strategies](#searching_strategies).\n",
    "6. [Working with GOPs](#GOP_concept).\n",
    "7. [MCTF (Motion Compensated Temporal Filtering)](#MCTF).\n",
    "8. [An algorithm for block-based framed interpolation](#linear_frame_interpolation).\n",
    "9. [Hybrid coding alternatives](#hybid_coding_alternatives).\n",
    "10. [Deblocking filtering](#deblocking).\n",
    "11. [Bit allocation strategies](#bit_allocation).\n",
    "12. [Video scalability](#scalabilities).\n",
    "13. [Encoding models](#models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sources'></a>\n",
    "## Sources of redundancy\n",
    "1. **Spatial redundancy**: Pixels are very similar in its neighborhood or tends to repeat textures.\n",
    "2. **Temporal redundancy**: Temporally adjacent images are typically very alike.\n",
    "3. **Visual redundancy**: Humans hardly perceive high spatial and temporal frequencies (we like more low frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MC'></a>\n",
    "## Block-based MC (Motion Compensation)\n",
    "\n",
    "* Usually, only performed by the encoder (compress one. decompress many).\n",
    "* MC removes temporal redundancy. A *predicted image* can be\n",
    "  encoded as the difference between it and another image called\n",
    "  *prediction image* which is a motion compensated projection of\n",
    "  one or more images named *reference images*. ME tries to\n",
    "  generate *residue images* as close as possible to the null\n",
    "  images.\n",
    "* For example, in the MPEG-1 standard, the reference image/s is/are divided in blocks of\n",
    "  $16\\times 16$ pixels called *macroblocks*.\n",
    "* Each reference block is searched in the predicted image and the\n",
    "  best match is indicated by mean of a *motion vector*.\n",
    "* Depending on the success of the search and the number of\n",
    "  reference images, the macroblocks are classified into:\n",
    "  + **I (intra)**: When the compression of residue block generates more\n",
    "    bits than the original (predicted) one.\n",
    "  + **P (predicted)**: When it is better to compress the residue block and\n",
    "    there is only one reference macroblock.\n",
    "  + **B (bidirectionally predicted)**: The same, but if we have two reference macroblocks.\n",
    "  + **S (skipped)**: When the energy of the residue block is\n",
    "    smaller than a given threshold.\n",
    "* I-pictures are composed of I macroblocks, only.\n",
    "* P-pictures do not have B macrobocks.\n",
    "* B-pictures can have any type of macroblocks.\n",
    "\n",
    "<img src=\"figs/macroblocks.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subpixel_accuracy'></a>\n",
    "## Sub-pixel accuracy\n",
    "\n",
    "* The motion estimation can be carried out using integer pixel\n",
    "  accuracy or a fractional (sub-) pixel accuracy.\n",
    "* For example, in MPEG-1, the motion estimation can have up to 1/2\n",
    "  pixel accuracy. A bi-linear interpolator is used:\n",
    "\n",
    "<img src=\"figs/interpolation.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='matching_criteria'></a>\n",
    "## Matching criteria (similitude between macroblocks)\n",
    "\n",
    "* Let $a$ and $b$ the macroblocks which we want to compare. Two\n",
    "  main distortion metrics are commonly used:\n",
    "  \n",
    "  + **MSE (Mean Square Error)**:\n",
    "  \n",
    "    \\begin{equation}\n",
    "      \\frac{1}{16\\times 16}\\sum_{i=1}^{16}\\sum_{j=1}^{16}(a_{ij}-b_{ij})^2\n",
    "    \\end{equation}\n",
    "    \n",
    "  + **MAE (Mean Absolute Error)**:\n",
    "  \n",
    "    \\begin{equation}\n",
    "      \\frac{1}{16\\times 16}\\sum_{i=1}^{16}\\sum_{j=1}^{16}|a_{ij}-b_{ij}|\n",
    "    \\end{equation}\n",
    "\n",
    "* These similitude measures are used only by MPEG\n",
    "  compressors. Therefore, any other one with similar effects (such as\n",
    "  the error variance or the error entropy) could be used also.\n",
    "\n",
    "* Other less common distortion metrics that can work are:\n",
    "\n",
    "  + **EE (Error [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))**:\n",
    "\n",
    "    \\begin{equation}\n",
    "      -\\frac{1}{16\\times 16}\\sum_{i=1}^{16}\\sum_{j=1}^{16}\\log_2(a_{ij}-b_{ij})p(a_{ij}-b_{ij})\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='searching_strategies'></a>\n",
    "## Searching strategies\n",
    "\n",
    "* Only performed by the compressor.\n",
    "\n",
    "    + **Full search**: All the possibilities are\n",
    "    checked. Advantage: the best compression. Disadvantage: CPU\n",
    "    killer.\n",
    "    \n",
    "    <img src=\"figs/full_search.svg\">\n",
    "\n",
    "    + ** Logaritmic search**: It is a version of the full search\n",
    "    algorithm where the macro-blocks and the search area are\n",
    "    sub-sampled. After finding the best coincidence, the resolution is increased in a power of 2 and the previous\n",
    "    match is refined in a search area of $\\pm 1$, until the maximal\n",
    "    resolution (even using subpixel accuracy) is reached.\n",
    "    \n",
    "    <a id='telescopic_search'></a>\n",
    "    + **Telescopic search**: Any of the previously described\n",
    "    techniques can be speeded up if the searching area is\n",
    "    reduced. This can be done supposing that the motion vector of the\n",
    "    same macro-block in two consecutive images is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='searching_strategies'></a>\n",
    "## The GOP (Group Of Pictures) concept\n",
    "\n",
    "* The temporal redundancy is exploited by blocks of images called\n",
    "  GOPs. This means that a GOP can be decoded independently of the rest\n",
    "  of GOPs. Here an example:\n",
    "  \n",
    "<img src=\"figs/GOPs.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MCTF'></a>\n",
    "## MCTF (Motion Compensated Temporal Filtering)\n",
    "\n",
    "* This is a DWT where the input samples are the original video\n",
    "  images and the output is a sequence of residue images.\n",
    "  \n",
    "<img src=\"figs/MCTF.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\pm 1$-spiral-search ME (Motion Estimation)\n",
    "<img src=\"figs/spiral_search.svg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_frame_interpolation'></a>\n",
    "## Linear frame interpolation using block-based motion compensation\n",
    "\n",
    "<img src=\"figs/frame_interpolation.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "* $R$: square search area, in pixels.\n",
    "* $B$: square block size, in pixels.\n",
    "* $O$: border size, in pixels.\n",
    "* $s_i$, $s_j$ and $s_k$ three chronologically ordered, equidistant frames, with resolution $X\\times Y$.\n",
    "* $A$: $\\frac{1}{2^A}$ subpixel accuracy.\n",
    "\n",
    "### Output\n",
    "\n",
    "* $\\hat{s}_j$: a prediction for frame $s_j$.\n",
    "* $m$: a matrix with $\\lceil X/B\\rceil \\times \\lceil Y/B\\rceil$ bidirectional motion vectors.\n",
    "* $e$: a matrix with $\\lceil X/B\\rceil \\times \\lceil Y/B\\rceil$ bidirectional Root Mean Square matching Wrrors (RMSE).\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Compute the DWT$^l$, where $l=\\lfloor\\log_2(R)\\rfloor-1$ levels, of the predicted frame $s_j$ and the two reference frames $s_i$ and $s_k$.\n",
    "  <img src=\"figs/frame_interpolation_step_1.svg\">\n",
    "2. $LL^l(m)\\leftarrow 0$, or any other precomputed values (for example, from a previous ME in neighbor frames).\n",
    "  <img src=\"figs/frame_interpolation_step_2.svg\" width=150>\n",
    "3. Divide the subband $LL^l(s_j)$ into blocks of size $B\\times B$ pixels, and $\\pm 1$-spiral-search them in the subbands $LL^l(s_i)$ and $LL^l(s_k)$, calculating a low-resolution $LL^l(m)=\\{LL^l(\\overleftarrow{m}), LL^l(\\overrightarrow{m})\\}$ bi-directional motion vector field.\n",
    "  <img src=\"figs/frame_interpolation_step_3A.svg\">\n",
    "  <img src=\"figs/frame_interpolation_step_3A_bis.svg\" width=200>\n",
    "4. While $l>0$:\n",
    "  1. Synthesize $LL^{l-1}(m)$, $LL^{l-1}(s_j)$, $LL^{l-1}(s_i)$ and $LL^{l-1}(s_k)$, by computing the 1-level DWT$^{-1}$.\n",
    "  <img src=\"figs/frame_interpolation_step_4A.svg\">\n",
    "  <img src=\"figs/frame_interpolation_step_4A_bis.svg\" width=200>\n",
    "  2. $LL^{l-1}(M)\\leftarrow LL^{l-1}(M)\\times 2$.\n",
    "  <img src=\"figs/frame_interpolation_step_4B.svg\" width=200>\n",
    "  3. Refine $LL^{l-1}(m)$ using $\\pm 1$-spiral-search.\n",
    "  <img src=\"figs/frame_interpolation_step_4C.svg\" width=200>\n",
    "  4. $l\\leftarrow l-1$. (When $l=0$, the motion vectors field $m$ has the structure:)\n",
    "  <img src=\"figs/motion_vectors.svg\" width=200>\n",
    "4. While $l<A$ (in the first iteration, $l=0$, and $LL^0(M):=M$):\n",
    "  1. $l\\leftarrow l+1$.\n",
    "  2. Synthesize $LL^{-l}(s_j)$, $LL^{-l}(s_i)$ and\n",
    "    $LL^{-l}(s_k)$, computing the 1-level DWT$^{-1}$ (high-frequency subbands are $0$). This performs a zoom-in in these frames using $1/2$-subpixel accuracy.\n",
    "    <img src=\"figs/frame_interpolation_step_5B.svg\">\n",
    "  3. $m\\leftarrow m\\times 2$.\n",
    "     <img src=\"figs/motion_vectors_by_2.svg\" width=210>\n",
    "  4. $B\\leftarrow B\\times 2$.\n",
    "  5. Divide the subband $LL^{-l}(s_j)$ into blocks of $B\\times B$ pixels\n",
    "    and $\\pm 1$-spiral-search them into the subbands $LL^{-l}(s_i)$\n",
    "    and $LL^{-l}(s_k)$, calculating a $1/2^l$ sub-pixel accuracy\n",
    "    $m$ bi-directional motion vector field.\n",
    "    <img src=\"figs/motion_vectors_definitive.svg\" width=280>\n",
    "1. Frame prediction. For each block $b$:\n",
    "  1. Compute\n",
    "    \\begin{equation}\n",
    "      \\hat{b}\\leftarrow \\frac{b_i\\big(\\overleftarrow{e}_\\text{max}-\\overleftarrow{e}(b)\\big) + b_k\\big(\\overrightarrow{e}_\\text{max}-\\overrightarrow{e}(b)\\big)}{\\big(\\overleftarrow{e}_\\text{max}-\\overleftarrow{e}(b)\\big) + \\big(\\overrightarrow{e}_\\text{max}-\\overrightarrow{e}(b)\\big)},\n",
    "    \\end{equation}\n",
    "    where $\\overleftarrow{e}(b)$ is the (minimum) distortion of the best backward matching for block $b$, $\\overrightarrow{e}(b)$ the (minimum) distortion of the best forward matching for block $b$, $\\overleftarrow{e}_\\text{max}=\\overrightarrow{e}_\\text{max}$ are the backward and forward maximum matching distortions, $b_i$ is the (backward) block found (as the most similar to $b$) in frame $s_i$ and $b_k$ is the (forward) block found in frame $s_k$. Notice that, if $\\overleftarrow{e}(b)=\\overrightarrow{e}(b)$, then the prediction is\n",
    "    \\begin{equation}\n",
    "      \\hat{b} = \\frac{b_i + b_k}{2},\n",
    "    \\end{equation}\n",
    "    and if $\\overleftarrow{e}(b)=0$,\n",
    "    \\begin{equation}\n",
    "      \\hat{b} = b_k,\n",
    "    \\end{equation}\n",
    "    and viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Implement the [frame predictor](#linear_frame_interpolation) (work on https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MC/BBBMC.py). Use https://github.com/Sistemas-Multimedia/MCDWT/blob/master/mcdwt/mc/block/interpolate.py and https://github.com/vicente-gonzalez-ruiz/MCTF-video-coding/blob/master/src/update.cpp as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Compare the performance of the proposed matching strategies (MSE, MAE and EE) in the [frame predictor](#linear_frame_interpolation), by computing the variance of the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Test different DWT filters in the [frame predictor](#linear_frame_interpolation) and compare their performance. Compute the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$). Measure the dependency between this performance and the distance between frames ($i$, $j$, and $k$ indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Test the use of both the luma and the chroma in [frame predictor](#linear_frame_interpolation), and measure the performance of each option (only luma vs. all components), by computing the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$). Measure the dependency of the results with the distance between frames ($i$, $j$, and $k$ indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Analyze the impact of the $R$ (search range) parameter in the [frame predictor](#linear_frame_interpolation). Compute the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$). Study the impact of initializing the motion vectors ([telescopic search](#telescopic_search)). Measure the dependency with the distance between frames ($i$, $j$, and $k$ indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Analyze the impact of the $O$ (overlaping) parameter in the [frame predictor](#linear_frame_interpolation), by means of computing the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$). Measure the dependency with the distance between frames ($i$, $j$, and $k$ indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Analyze the impact of the $B$ (block size) parameter in the [frame predictor](#linear_frame_interpolation), by computing the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$). Compute the expected size of the motion fields using their 0-order entropy. Measure the dependency with the distance between frames ($i$, $j$, and $k$ indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Analyze the impact of the $A$ (subpixel accuracy) parameter in the [frame predictor](#linear_frame_interpolation), by computing the prediction error between the original frame ($s_j$) and the prediction frame ($\\hat{s}_j$). Compute the expected size of the motion fields using their entropy. Measure the dependency with the distance between frames ($i$, $j$, and $k$ indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Compare the performance of the [frame predictor](#linear_frame_interpolation) when it holds that\n",
    "\n",
    "\\begin{equation}\n",
    "   \\hat{b} = \\frac{b_i + b_k}{2},\n",
    "\\end{equation}\n",
    "\n",
    "for all blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hybid_coding_alternatives'></a>\n",
    "## MC/DWT hybrid coding alternatives\n",
    "\n",
    "* **t+2d**: The sequence of images is decorrelated first\n",
    "  along the time (t) and the residue images are compressed, exploiting\n",
    "  the remaining spatial (2d) redundancy. Examples: MPEG* and H.26*\n",
    "  codecs (except H.264/SVC).\n",
    "  \n",
    "* **2d+t**: The spatial (2d) redudancy is explited first\n",
    "  (using typically the DWT) and after that, the coefficients are decorrelated\n",
    "  along the time (t). For now, this has only been an experimental setup\n",
    "  because most DWT transformed domains are not invariant to the\n",
    "  displacement, and therefore, ME/MC can not be directly applied.\n",
    "  \n",
    "* **2d+t+2d**: The fist step creates a Laplacian Pyramid\n",
    "  (2d), which is invariant to the displacement. Next, each level of\n",
    "  the pyramid is decorrelated along the time (t) and finally, the\n",
    "  remaining spatial redundancy is removed (2d). Example: H.264/SVC.\n",
    "\n",
    "<img src=\"figs/H264-S-SVC.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deblocking'></a>\n",
    "## Deblocking filtering\n",
    "\n",
    "* If any other block-overlaping techniques have not been applied, block-based video encoders improve their performance if a deblocking filter in used to create the quantized prediction predictions.\n",
    "  \n",
    "<img src=\"figs/350px-Deblock1.jpg\" width=600>\n",
    "\n",
    "* The low-pass filter is applied only on the block boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bit_allocation'></a>\n",
    "## Bit-rate allocation\n",
    "\n",
    "* VBR: Under a constant quantization level (constant video quality),\n",
    "  the number of bits that each compressed image needs depends on the\n",
    "  image content (Variable Bit-Rate). Example:\n",
    "\n",
    "  <img src=\"figs/closed-loop-1_ir.svg\">\n",
    "  \n",
    "* CBR: Using a Constant Bit-Rate strategy, all frames need the same space. Example:\n",
    "\n",
    "  <img src=\"figs/CBR.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scalabilities'></a>\n",
    "## Video scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality scalability\n",
    "\n",
    "<img src=\"figs/quality-scalability.svg\">\n",
    "\n",
    "* Ideal for remote visualization environments.\n",
    "\n",
    "* By definition, $s^{[0]}:=s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal scalability\n",
    "\n",
    "<img src=\"figs/temporal-scalability.svg\">\n",
    "\n",
    "* It holds that\n",
    "\n",
    "  \\begin{equation}\n",
    "    s^{t}=\\{s_{2^t i}\\}=\\{s_{2i}^{t-1}\\},\n",
    "  \\end{equation}\n",
    "  \n",
    "  where $t$ denotes the Temporal Resolution Level (TRL).\n",
    "\n",
    "* Notice that $s:=s^{0}$.\n",
    "\n",
    "* Useful for fast random access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial scalability\n",
    "\n",
    "  <img src=\"figs/spatial-scalability.svg\">\n",
    "\n",
    "* Useful for low-resolution devices.\n",
    "\n",
    "* By definition, $V_i:=V_i^{<0>}$ and $V_i^{<s>}$ has a\n",
    "  $\\frac{Y}{2^s}\\times \\frac{X}{2^s}$ resolution, where $X\\times Y$ is\n",
    "  the resolution of $V_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='models'></a>\n",
    "## Media encoding models\n",
    "\n",
    "<img src=\"figs/media-encoding-models.svg\" width=400>\n",
    "\n",
    "#### Single Layer Coding (SLC)\n",
    "\n",
    "* Most audio and image/video codecs generate non-scalable\n",
    "  streams. In the case of video, only one quality, resolution and\n",
    "  picture-rate are available at decoding time.\n",
    "  \n",
    "* The decoding of a single layered stream generates a\n",
    "  reconstruction whose quality is linearly proportional to the amount\n",
    "  of decoded data.\n",
    "  \n",
    "#### [Multiple Layer Coding (MLC)](http://eeweb.poly.edu/~yao/EL6123/scalablecoding.pdf)\n",
    "\n",
    "* A media encoded in several layers can be decoded to provide (in\n",
    "  the case of video) different picture-rates (time scalability),\n",
    "  different resolutions (spatial scalability) and different qualities\n",
    "  (quality scalability).\n",
    "  \n",
    "* In some codecs (such as JPEG 2000), spatial random access it is\n",
    "  available ROI (Region-Of-Interest) or WOI (Window-Of-Interest)\n",
    "  scalability. ROI is used in special imaging, such as [mammography](http://en.wikipedia.org/wiki/Mammography). WOI can\n",
    "  useful in the retrieving of high-resolution video sequences such as [JHelioviewer](http://jhelioviewer.org/linux.html).\n",
    "  \n",
    "#### [Multiple Description Coding (MDC)](http://en.wikipedia.org/wiki/Multiple_description_coding)\n",
    "\n",
    "* Multiple description codecs provides a set of\n",
    "  partially redundant streams so that the quality of the reconstructions\n",
    "  improve with the number of descriptions decoded.\n",
    "  \n",
    "* An example of this type of encoding is the scene segmentation\n",
    "  (video object coding) provided by [MPEG-4](http://www.cs.cf.ac.uk/Dave/MM/BSC_MM_CALLER/PDF/12_CM340_MPEG4_VIDEO.pdf).\n",
    "  \n",
    "#### [Media simulcast](http://en.wikipedia.org/wiki/Simulcast)\n",
    "\n",
    "* In transmission scenarios, a source can store several copies of\n",
    "  the same media, althought variying the temporal resolution, spatial\n",
    "  resolution and/or quality.\n",
    "  \n",
    "* Obviously, this is quite redundant at the source side. However,\n",
    "  adaptive services can be provided with\n",
    "  this technique, such as in YouTube which uses [DASH](https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
